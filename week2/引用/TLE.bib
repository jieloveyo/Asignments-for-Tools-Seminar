
@inproceedings{deng_tie:_2019,
	location = {New York, {NY}, {USA}},
	title = {{TIE}: Energy-efficient Tensor Train-based Inference Engine for Deep Neural Network},
	isbn = {978-1-4503-6669-4},
	url = {http://doi.acm.org/10.1145/3307650.3322258},
	doi = {10.1145/3307650.3322258},
	series = {{ISCA} '19},
	shorttitle = {{TIE}},
	abstract = {In the era of artificial intelligence ({AI}), deep neural networks ({DNNs}) have emerged as the most important and powerful {AI} technique. However, large {DNN} models are both storage and computation intensive, posing significant challenges for adopting {DNNs} in resource-constrained scenarios. Thus, model compression becomes a crucial technique to ensure wide deployment of {DNNs}. This paper advances the state-of-the-art by considering tensor train ({TT}) decomposition, an very promising but yet explored compression technique in architecture domain. The method features with the extremely high compression ratio. However, the challenge is that the inference on the {TT}-format {DNN} models inherently incurs massive amount of redundant computations, causing significant energy consumption. Thus, the straightforward application of {TT} decomposition is not feasible. To address this fundamental challenge, this paper develops a computation-efficient inference scheme for {TT}-format {DNN}, which enjoys two key merits: 1) it achieves theoretical limit of number of multiplications, thus eliminating all redundant computations; and 2) the multi-stage processing scheme reduces the intensive memory access to all tensor cores, bringing significant energy saving. Based on the novel inference scheme, we develop {TIE}, a {TT}-format compressed {DNN}-targeted inference engine. {TIE} is highly flexible, supporting different types of networks for different needs. A 16-processing elements ({PE}) prototype is implemented using {CMOS} 28nm technology. Operating on 1000MHz, the {TIE} accelerator consumes 1.74mm2 and 154.8mW. Compared with {EIE}, {TIE} achieves 7.22× {\textasciitilde} 10.66× better area efficiency and 3.03× {\textasciitilde} 4.48× better energy efficiency on different workloads, respectively. Compared with {CirCNN}, {TIE} achieves 5.96× and 4.56× higher throughput and energy efficiency, respectively. The results show that {TIE} exhibits significant advantages over state-of-the-art solutions.},
	pages = {264--278},
	booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
	publisher = {{ACM}},
	author = {Deng, Chunhua and Sun, Fangxuan and Qian, Xuehai and Lin, Jun and Wang, Zhongfeng and Yuan, Bo},
	urldate = {2019-11-26},
	date = {2019},
	note = {event-place: Phoenix, Arizona},
	keywords = {acceleration, compression, deep learning, tensor-train decomposition},
	file = {ACM Full Text PDF:C\:\\Users\\蓝梦\\Zotero\\storage\\LSVG6YR9\\Deng 等。 - 2019 - TIE Energy-efficient Tensor Train-based Inference.pdf:application/pdf}
}