## Simple summary of the paper
    Energy-efficient Tensor Train-based Inference Engine for Deep Neural Network  

In AI,deep neural networks(DNNs) have emerged as the most important and powerful AI technique. 
However, large DNN models are both storage and computation intensive,posing significant challenges for adopting DNNs in resource-constrained scenarios. 

**So this paper is tend to simplify the large DNN models.**

Model compression is a crucial technique to ensure wide deployment of DNNs.Tensor train (TT) decomposition is one of technique.But it still has many problems unsolved.

**Thus, this paper is tend to solve some of the problems and make it feasible**

This paper advances the state-of-the-art by considering tensor train (TT) decomposition and develops a computation-efficient inference scheme for TT-format DNN.In addition, based on the novel inference scheme, they develop TIE, a TTformat compressed DNN-targeted inference engine.


